<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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"/>
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K"/>
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg=="/>
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_XNW9IFES" class="item bookSection">
			<h2>A Geometric Perspective on Optimal Representations for Reinforcement Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marc Bellemare</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Will Dabney</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Dadashi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adrien Ali Taiga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pablo Samuel Castro</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolas Le Roux</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dale Schuurmans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tor Lattimore</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Clare Lyle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Larochelle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>A. Beygelzimer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>F. d\textquotesingle Alché-Buc</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>E. Fox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/8687-a-geometric-perspective-on-optimal-representations-for-reinforcement-learning.pdf">http://papers.nips.cc/paper/8687-a-geometric-perspective-on-optimal-representations-for-reinforcement-learning.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>4358–4369</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BellemareGeometric2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:44:29 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 32</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:44:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_95FE69A6">BellemareGeometric2019.pdf					</li>
					<li id="item_KEHY4LMD">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_75XL6JSD" class="item journalArticle">
			<h2>A Metric Learning Reality Check</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Musgrave</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Serge Belongie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ser-Nam Lim</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.08505">http://arxiv.org/abs/2003.08505</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.08505 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-18</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.08505
version: 1
Citation Key: MusgraveMetric2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 8:01:26 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental setup of these papers, and propose a new way to evaluate metric learning algorithms. Finally, we present experimental results that show that the improvements over time have been marginal at best.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 8:01:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:23 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MMVYBQ8I">arXiv.org Snapshot					</li>
					<li id="item_W78G2IVL">MusgraveMetric2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_W78G2IVL/1">A Metric Learning Reality Check</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_44XQY3GD" class="item journalArticle">
			<h2>A Simple Framework for Contrastive Learning of Visual Representations</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ting Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon Kornblith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Norouzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Hinton</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2002.05709">http://arxiv.org/abs/2002.05709</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2002.05709 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-13</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2002.05709
Citation Key: ChenSimple2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/18/2020, 5:18:22 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/18/2020, 5:18:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:30 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NEG4IWDP">arXiv.org Snapshot					</li>
					<li id="item_TVCREK6P">ChenSimple2020.pdf					</li>
				</ul>
			</li>


			<li id="item_8V348BLN" class="item conferencePaper">
			<h2>A Theoretical Analysis of Contrastive Unsupervised Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikunj Saunshi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Orestis Plevrakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjeev Arora</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mikhail Khodak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hrishikesh Khandeparkar</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://proceedings.mlr.press/v97/saunshi19a.html">http://proceedings.mlr.press/v97/saunshi19a.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>5628-5637</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019/05/24</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 1938-7228
Section: Machine Learning
Citation Key: SaunshiTheoretical2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/5/2020, 9:55:59 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>proceedings.mlr.press</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>International Conference on Machine Learning</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of...</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>International Conference on Machine Learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/5/2020, 9:55:59 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:22 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PVWS9455">SaunshiTheoretical2019.pdf					</li>
				</ul>
			</li>


			<li id="item_8I5NYU39" class="item journalArticle">
			<h2>AutoAugment: Learning Augmentation Policies from Data</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ekin D. Cubuk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Barret Zoph</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dandelion Mane</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vijay Vasudevan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Quoc V. Le</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1805.09501">http://arxiv.org/abs/1805.09501</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1805.09501 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-04-11</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1805.09501
Citation Key: CubukAutoAugment2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 4:29:39 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AutoAugment</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 4:29:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_E9UIPBW9">
<p class="plaintext">Comment: CVPR 2019</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UAS7BRBQ">arXiv.org Snapshot					</li>
					<li id="item_MS4LLJT6">CubukAutoAugment2019.pdf					</li>
				</ul>
			</li>


			<li id="item_64DYKNYS" class="item journalArticle">
			<h2>Automatic Shortcut Removal for Self-Supervised Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthias Minderer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olivier Bachem</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neil Houlsby</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Tschannen</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2002.08822">http://arxiv.org/abs/2002.08822</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2002.08822 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-21</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2002.08822
Citation Key: MindererAutomatic2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/20/2020, 5:57:21 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In self-supervised visual representation learning, a feature extractor is trained on a &quot;pretext task&quot; for which labels can be generated cheaply. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such &quot;shortcut&quot; features and hand-designing schemes to reduce their effect. Here, we propose a general framework for removing shortcut features automatically. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a &quot;lens&quot; network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:57:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X3HYLCY9">arXiv.org Snapshot					</li>
					<li id="item_6JKNQK5H">MindererAutomatic2020.pdf					</li>
				</ul>
			</li>


			<li id="item_RV6XYW4E" class="item journalArticle">
			<h2>Big Self-Supervised Models are Strong Semi-Supervised Learners</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ting Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon Kornblith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Swersky</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Norouzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Hinton</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2006.10029">http://arxiv.org/abs/2006.10029</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2006.10029 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-06-17</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2006.10029</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>7/5/2020, 8:46:03 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/5/2020, 8:46:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/5/2020, 8:46:03 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_C63HUSPT">
<p class="plaintext">Comment: code and pretrained models at https://github.com/google-research/simclr</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_H9CL6IA4">Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_H9CL6IA4/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/2">2 Method</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H9CL6IA4/4">3 Empirical Study</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/4">3.1 Settings and Implementation Details</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/5">3.2 Bigger Models Are More Label-Efficient</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/6">3.3 Bigger/Deeper Projection Heads Improve Representation Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/7">3.4 Distillation Using Unlabeled Data Improves Semi-Supervised Learning</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/7">4 Related work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/8">5 Discussion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/9">6 Broader Impact</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/13">A When Do Bigger Models Help More?</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/13">B Parameter Efficiency Also Matters</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/13">C The Correlation Between Linear Evaluation and Fine-tuning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/14">D The Impact of Memory</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/14">E The Impact of Projection Head Under Different Model Sizes</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/15">F Further Distillation Ablations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H9CL6IA4/16">G Extra Results</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_B8X2XL9V" class="item conferencePaper">
			<h2>Boosting Few-Shot Visual Learning With Self-Supervision</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Spyros Gidaris</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrei Bursuc</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikos Komodakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Patrick Perez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthieu Cord</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.html">http://openaccess.thecvf.com/content_ICCV_2019/html/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>8059-8068</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: GidarisBoosting2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 11:42:32 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE International Conference on Computer Vision</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 11:42:31 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DPUQAS5P">GidarisBoosting2019.pdf					</li>
					<li id="item_QTT8FDNI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CVUUTNY2" class="item journalArticle">
			<h2>Boosting Supervision with Self-Supervision for Few-shot Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jong-Chyi Su</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Subhransu Maji</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bharath Hariharan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1906.07079">http://arxiv.org/abs/1906.07079</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1906.07079 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-06-17</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1906.07079
Citation Key: SuBoosting2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 11:43:42 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a technique to improve the transferability of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. While recent approaches for self-supervised learning have shown the benefits of training on large unlabeled datasets, we find improvements in generalization even on small datasets and when combined with strong supervision. Learning representations with self-supervised losses reduces the relative error rate of a state-of-the-art meta-learner by 5-25% on several few-shot learning benchmarks, as well as off-the-shelf deep networks on standard classification tasks when training from scratch. We find the benefits of self-supervision increase with the difficulty of the task. Our approach utilizes the images within the dataset to construct self-supervised losses and hence is an effective way of learning transferable representations without relying on any external training data.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 11:43:42 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8TVMBQFP">arXiv.org Snapshot					</li>
					<li id="item_IIF27UKX">SuBoosting2019.pdf					</li>
				</ul>
			</li>


			<li id="item_2HH6XFVM" class="item journalArticle">
			<h2>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean-Bastien Grill</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Florian Strub</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Florent Altché</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Corentin Tallec</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pierre H. Richemond</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elena Buchatskaya</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carl Doersch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernardo Avila Pires</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhaohan Daniel Guo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Gheshlaghi Azar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bilal Piot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Koray Kavukcuoglu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rémi Munos</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michal Valko</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2006.07733">http://arxiv.org/abs/2006.07733</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2006.07733 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-06-13</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2006.07733</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/26/2020, 9:11:23 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classiﬁcation accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Bootstrap Your Own Latent</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/26/2020, 9:11:23 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/26/2020, 9:11:24 AM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P7IA5ELR">Grill et al. - 2020 - Bootstrap Your Own Latent A New Approach to Self-.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_P7IA5ELR/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/2">2 Related work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_P7IA5ELR/3">3 Method</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/3">3.1 Description of BYOL</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/4">3.2 Implementation details</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/5">4 Experimental evaluation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/7">5 Building intuitions with ablations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/9">6 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/13">A Algorithm</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/13">B Image augmentations</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_P7IA5ELR/14">C Evaluation on ImageNet training</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/14">C.1 Self-supervised learning evaluation on ImageNet</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/16">C.2 Linear evaluation on larger architectures and supervised baselines</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_P7IA5ELR/18">D Transfer to other datasets</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/18">D.1 Datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/18">D.2 Transfer via linear classification</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/19">D.3 Transfer via fine-tuning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/19">D.4 Implementation details for semantic segmentation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/19">D.5 Implementation details for object detection </a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/19">D.6 Implementation details for depth estimation </a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/20">D.7 Further comparisons on PASCAL and NYU v2 Depth</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/20">E Pretraining on Places 365</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_P7IA5ELR/21">F Additional ablation results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/21">F.1 Architecture settings</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/21">F.2 Batch size</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/22">F.3 Image augmentations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/22">F.4 Details on the relation to contrastive methods</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/23">F.5 SimCLR baseline of sec:contrast</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_P7IA5ELR/24">F.6 Ablation on the normalization in the loss function</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_2WVYGAD3" class="item conferencePaper">
			<h2>Context Encoders: Feature Learning by Inpainting</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Deepak Pathak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Krahenbuhl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeff Donahue</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Trevor Darrell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexei A. Efros</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html">http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2536-2544</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: PathakContext2016</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2020, 10:13:36 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Context Encoders</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/25/2020, 10:13:36 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5HGT8CDY">PathakContext2016.pdf					</li>
					<li id="item_3JSYQ8YV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_D2ES3BHM" class="item journalArticle">
			<h2>Contrastive estimation reveals topic posterior information to linear models</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Tosh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Akshay Krishnamurthy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Hsu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.02234">http://arxiv.org/abs/2003.02234</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.02234 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-04</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.02234
Citation Key: ToshContrastive2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/24/2020, 12:03:00 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/24/2020, 12:03:00 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I2S5AUVG">arXiv.org Snapshot					</li>
					<li id="item_A63NPZN9">ToshContrastive2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_A63NPZN9/1">1 Introduction</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/2">1.1 Related work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/3">1.2 Overview of results</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/3">2 Setup</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/3">3 Contrastive learning algorithm</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_A63NPZN9/5">4 Recovering topic structure</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/5">4.1 The single topic case</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/6">4.2 The general setting</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/8">5 Error analysis</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_A63NPZN9/10">6 Semi-supervised experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/10">6.1 A closely related representation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/11">6.2 Methodology</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/11">6.3 Results</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_A63NPZN9/13">7 Topic modeling simulations</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/13">7.1 Simulation setup</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/14">7.2 Results</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/14">8 Discussion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_A63NPZN9/17">A Proofs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/17">A.1 Proof of general representation lemma</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_A63NPZN9/17">A.2 Error analysis</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_V5DZV53G" class="item journalArticle">
			<h2>Contrastive multiview coding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yonglong Tian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dilip Krishnan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phillip Isola</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1906.05849</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: TianContrastive2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:48:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5BBHPMGV">Snapshot					</li>
					<li id="item_TXZPNNTC">TianContrastive2019.pdf					</li>
				</ul>
			</li>


			<li id="item_QI2ZUYBY" class="item journalArticle">
			<h2>Contrastive Representation Distillation</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yonglong Tian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dilip Krishnan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phillip Isola</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1910.10699">http://arxiv.org/abs/1910.10699</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1910.10699 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-18</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1910.10699
Citation Key: TianContrastive2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/24/2020, 12:01:53 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher&apos;s representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/24/2020, 12:01:53 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_X6XI5NWL">
<p class="plaintext">Comment: ICLR 2020. Project Page: http://hobbitlong.github.io/CRD/, Code: http://github.com/HobbitLong/RepDistiller</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WLS49DTU">arXiv.org Snapshot					</li>
					<li id="item_PDWALBI6">TianContrastive2020.pdf					</li>
				</ul>
			</li>


			<li id="item_WMADHI9M" class="item journalArticle">
			<h2>Data Transformation Insights in Self-supervision with Clustering Tasks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abhimanu Kumar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aniket Anand Deshmukh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Urun Dogan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Denis Charles</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eren Manavoglu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2002.07384">http://arxiv.org/abs/2002.07384</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2002.07384 [cs, math, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-18</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2002.07384
Citation Key: KumarData2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/24/2020, 12:05:12 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Self-supervision is key to extending use of deep learning for label scarce domains. For most of self-supervised approaches data transformations play an important role. However, up until now the impact of transformations have not been studied. Furthermore, different transformations may have different impact on the system. We provide novel insights into the use of data transformation in self-supervised tasks, specially pertaining to clustering. We show theoretically and empirically that certain set of transformations are helpful in convergence of self-supervised clustering. We also show the cases when the transformations are not helpful or in some cases even harmful. We show faster convergence rate with valid transformations for convex as well as certain family of non-convex objectives along with the proof of convergence to the original set of optima. We have synthetic as well as real world data experiments. Empirically our results conform with the theoretical insights provided.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/24/2020, 12:05:12 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Mathematics - Statistics Theory</li>
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9P4PAI7P">arXiv.org Snapshot					</li>
					<li id="item_59K86ESM">KumarData2020.pdf					</li>
				</ul>
			</li>


			<li id="item_NZEIZH6E" class="item journalArticle">
			<h2>Data-Efficient Image Recognition with Contrastive Predictive Coding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olivier J. Hénaff</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aravind Srinivas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeffrey De Fauw</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ali Razavi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carl Doersch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. M. Ali Eslami</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron van den Oord</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1905.09272">http://arxiv.org/abs/1905.09272</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1905.09272 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-12-06</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1905.09272
Citation Key: HenaffDataEfficient2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/20/2020, 5:46:10 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on PASCAL VOC-2007, surpassing fully supervised pre-trained ImageNet classifiers.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:46:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8CP5EWAH">arXiv.org Snapshot					</li>
					<li id="item_469EJY35">HenaffDataEfficient2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_469EJY35/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_469EJY35/2">2 Experimental Setup</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/2">2.1 Contrastive Predictive Coding</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/3">2.2 Evaluation protocol</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/4">3 Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_469EJY35/5">4 Results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/5">4.1 From CPC v1 to CPC v2</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/7">4.2 Efficient image classification</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/8">4.3 Transfer learning: image detection on PASCAL VOC 2007</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/9">5 Discussion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_469EJY35/14">A Appendix</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/14">A.1 Additional Results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/14">A.2 InfoNCE implementation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/15">A.3 Linear classification</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_469EJY35/15">A.4 Efficient classification: purely supervised</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_AM3ABSQJ" class="item conferencePaper">
			<h2>Deep clustering for unsupervised learning of visual features</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mathilde Caron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Piotr Bojanowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Armand Joulin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthijs Douze</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>132–149</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: CaronDeep2018</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the European Conference on Computer Vision (ECCV)</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2020, 5:04:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/29/2020, 12:46:01 AM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_S3TXNU82">CaronDeep2018.pdf					</li>
					<li id="item_CPJZFIK6">Snapshot					</li>
				</ul>
			</li>


			<li id="item_94GR6VS9" class="item journalArticle">
			<h2>Deep graph infomax</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petar Veličković</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Fedus</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William L. Hamilton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pietro Liò</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Devon Hjelm</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1809.10341</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: VelickovicDeep2018</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:53:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SPLK3YWN">Snapshot					</li>
					<li id="item_SQUYZD9Z">VelickovicDeep2018.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_SQUYZD9Z/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/2">2 Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_SQUYZD9Z/3">3 DGI Methodology</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/3">3.1 Graph-based unsupervised learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/3">3.2 Local-global mutual information maximization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/4">3.3 Theoretical motivation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/5">3.4 Overview of DGI</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_SQUYZD9Z/6">4 Classification performance</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/6">4.1 Datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/6">4.2 Experimental setup</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/8">4.3 Results</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/10">5 Qualitative analysis</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/10">6 Conclusions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/14">A Further dataset details</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/14">B Further qualitative analysis</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_SQUYZD9Z/16">C Robustness to Choice of Corruption Function</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_ICKP4T7Z" class="item journalArticle">
			<h2>Deep learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yann LeCun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Hinton</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>521</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>7553</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>436–444</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>nature</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2015</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Nature Publishing Group
Citation Key: LeCunDeep2015</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/15/2020, 11:25:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:16 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_24E45FXG">LeCunDeep2015.pdf					</li>
				</ul>
			</li>


			<li id="item_5QNV8242" class="item journalArticle">
			<h2>DeepMDP: Learning Continuous Latent Space Models for Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carles Gelada</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saurabh Kumar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob Buckman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ofir Nachum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marc G. Bellemare</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1906.02736">http://arxiv.org/abs/1906.02736</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1906.02736 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-06-06</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1906.02736
Citation Key: GeladaDeepMDP2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:44:05 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>DeepMDP</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:44:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_FES7JCKJ">
<p class="plaintext">Comment: 13 pages main text, 16 pages appendix. ICML 2019</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZR6RPMNG">arXiv.org Snapshot					</li>
					<li id="item_RUEHAJXB">GeladaDeepMDP2019.pdf					</li>
				</ul>
			</li>


			<li id="item_27XMBW8N" class="item journalArticle">
			<h2>Demystifying Self-Supervised Learning: An Information-Theoretical Framework</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yao-Hung Hubert Tsai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yue Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruslan Salakhutdinov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Louis-Philippe Morency</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2006.05576">http://arxiv.org/abs/2006.05576</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2006.05576 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-06-10</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2006.05576</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/19/2020, 6:44:15 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as masked language modeling (e.g., BERT) for natural language processing and contrastive visual representation learning (e.g., SimCLR) for computer vision applications. In this paper, we present a theoretical framework explaining that self-supervised learning is likely to work under the assumption that only the shared information (e.g., contextual information or content) between the input (e.g., non-masked words or original images) and self-supervised signals (e.g., masked-words or augmented images) contributes to downstream tasks. Under this assumption, we demonstrate that self-supervisedly learned representation can extract task-relevant and discard task-irrelevant information. We further connect our theoretical analysis to popular contrastive and predictive (self-supervised) learning objectives. In the experimental section, we provide controlled experiments on two popular tasks: 1) visual representation learning with various self-supervised learning objectives to empirically support our analysis; and 2) visual-textual representation learning to challenge that input and self-supervised signal lie in different modalities.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Demystifying Self-Supervised Learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/19/2020, 6:44:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/19/2020, 6:44:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PQF63D5X">arXiv.org Snapshot					</li>
					<li id="item_WLNMKKKW">TsaiDemystifying2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_WLNMKKKW/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_WLNMKKKW/2">2 An Information-Theoretical Framework for Self-supervised Learning</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/2">2.1 Redundancy Assumption and Determinism</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/3">2.2 Supervised Representation Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/3">2.3 A Self-supervised Representation Learning Strategy</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/4">2.4 Relations with Contrastive and Predictive Representation Learning</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/6">3 Controlled Experiments</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/8">4 Related Work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/8">5 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/11">6 Proofs for Lemmas</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/11">7 Information Diagram Road Map</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_WLNMKKKW/12">8 More on Visual Representation Learning Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/12">8.1 Architecture Design</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/12">8.2 Different Deployments for Contrastive and Predictive Learning Objectives</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/13">8.3 Different Self-supervised Signal Construction Strategy</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_WLNMKKKW/14">9 Metrics in Visual-Textual Representation Learning</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_E5GF8SBG" class="item conferencePaper">
			<h2>Dimensionality Reduction by Learning an Invariant Mapping</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Hadsell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. Chopra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Y. LeCun</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1735-1742</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>June 2006</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 1063-6919
Citation Key: HadsellDimensionality2006</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/CVPR.2006.100">10.1109/CVPR.2006.100</a></td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that &apos;similar&quot; points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:24:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:29 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Image analysis</li>
					<li>Extraterrestrial measurements</li>
					<li>Astronomy</li>
					<li>Biology</li>
					<li>Data visualization</li>
					<li>Feature extraction</li>
					<li>Geoscience</li>
					<li>Image generation</li>
					<li>Manufacturing industries</li>
					<li>Service robots</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_C6RNW2QG">HadsellDimensionality2006.pdf					</li>
					<li id="item_RK5A64PC">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_L3ITIGNM" class="item bookSection">
			<h2>Discriminative Unsupervised Feature Learning with Convolutional Neural Networks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexey Dosovitskiy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jost Tobias Springenberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin Riedmiller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Brox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Z. Ghahramani</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>M. Welling</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>C. Cortes</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>N. D. Lawrence</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>K. Q. Weinberger</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>766–774</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2014</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: DosovitskiyDiscriminative2014</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/20/2020, 5:27:28 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 27</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:27:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:29 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_U62M6SQF">DosovitskiyDiscriminative2014.pdf					</li>
					<li id="item_Y7BQLW2X">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_9GA3WZU5" class="item journalArticle">
			<h2>Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexey Dosovitskiy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Fischer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jost Tobias Springenberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin Riedmiller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Brox</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>38</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1734-1747</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1939-3539</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>Sep. 2016</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence
Citation Key: DosovitskiyDiscriminative2016</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/TPAMI.2015.2496141">10.1109/TPAMI.2015.2496141</a></td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled `seed&apos; image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While features learned with our approach cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:26:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:29 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Training</li>
					<li>learning (artificial intelligence)</li>
					<li>image classification</li>
					<li>computer vision</li>
					<li>feature extraction</li>
					<li>Neural networks</li>
					<li>Accuracy</li>
					<li>neural nets</li>
					<li>Feature extraction</li>
					<li>computer vision tasks</li>
					<li>Convolutional networks</li>
					<li>descriptor matching</li>
					<li>discriminative unsupervised feature learning</li>
					<li>exemplar convolutional neural networks</li>
					<li>feature learning</li>
					<li>generic feature learning</li>
					<li>generic feature representation</li>
					<li>geometric matching problems</li>
					<li>Image color analysis</li>
					<li>image matching</li>
					<li>image patch</li>
					<li>image representation</li>
					<li>SIFT descriptor</li>
					<li>supervised learning</li>
					<li>supervised network training</li>
					<li>Support vector machines</li>
					<li>transforms</li>
					<li>unlabeled data</li>
					<li>unsupervised learning</li>
					<li>Unsupervised learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_H84JP8UD">DosovitskiyDiscriminative2016.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_H84JP8UD/1">1 Introduction</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/1">1.1 Related Work</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/2">2 Creating Surrogate Training Data</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H84JP8UD/2">3 Learning Algorithm</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/3">3.1 Formal Analysis</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/3">3.2 Conceptual Comparison to Previous Unsupervised Learning Methods</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H84JP8UD/4">4 Experiments: Classification</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/4">4.1 Experimental Setup</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/4">4.2 Classification Results</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H84JP8UD/5">4.3 Detailed Analysis</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/5">4.3.1 Number of Surrogate Classes</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/5">4.3.2 Number of Samples per Surrogate Class</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/5">4.3.3 Types of Transformations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/6">4.3.4 Influence of the Dataset</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/6">4.3.5 Influence of the Network Architecture on Classification Performance</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/7">4.3.6 Invariance Properties of the Learned Representation</a></li></ul></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H84JP8UD/8">5 Experiments: Descriptor Matching</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/8">5.1 Compared Features</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/8">5.2 Datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/9">5.3 Performance Measure</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/9">5.4 Patch size and network layer</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/9">5.5 Results</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/10">6 Conclusions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/12">Appendix A: Formal analysis</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_H84JP8UD/12">Appendix B: Method details</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/12">B.1 Network Architecture</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/12">B.2 Training the Networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/12">B.3 Clustering</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_H84JP8UD/12">Appendix C: Details of computing the measure of invariance</a></li></ul></div>
					</div>					</li>
					<li id="item_UHPLMIB7">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_RWNAV5EM" class="item conferencePaper">
			<h2>Does Object Recognition Work for Everyone?</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Terrance de Vries</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishan Misra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Changhan Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laurens van der Maaten</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>52–59</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: deVriesDoes2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:50:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B8CESN9N">deVriesDoes2019.pdf					</li>
					<li id="item_WD7H9KE7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_6CCUDTQX" class="item journalArticle">
			<h2>Entropy-SGD: Biasing Gradient Descent Into Wide Valleys</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pratik Chaudhari</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna Choromanska</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefano Soatto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yann LeCun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carlo Baldassi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Borgs</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Chayes</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Levent Sagun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Riccardo Zecchina</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1611.01838">http://arxiv.org/abs/1611.01838</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1611.01838 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017-04-21</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1611.01838</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>7/2/2020, 5:20:06 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Entropy-SGD</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/2/2020, 5:20:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/2/2020, 5:20:07 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_HDFD2VJ6">
<p class="plaintext">Comment: ICLR &apos;17</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZN58SCAC">arXiv Fulltext PDF					</li>
					<li id="item_RNVPIHBS">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_LKVT7Q2E" class="item journalArticle">
			<h2>Few-shot Action Recognition via Improved Attention with Self-supervision</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hongguang Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Li Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaojuan Qi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hongdong Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip H. S. Torr</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Piotr Koniusz</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2001.03905">http://arxiv.org/abs/2001.03905</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2001.03905 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-12</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2001.03905
Citation Key: ZhangFewshot2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 12:47:10 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Most existing few-shot learning methods in computer vision focus on class recognition given a few of still images as the input. In contrast, this paper tackles a more challenging task of few-shot action-recognition from video clips. We propose a simple framework which is both flexible and easy to implement. Our approach exploits joint spatial and temporal attention mechanisms in conjunction with self-supervised representation learning on videos. This design encourages the model to discover and encode spatial and temporal attention hotspots important during the similarity learning between dynamic video sequences for which locations of discriminative patterns vary in the spatio-temporal sense. Our method compares favorably with several state-of-the-art baselines on HMDB51, miniMIT and UCF101 datasets, demonstrating its superior performance.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 12:47:11 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IM529Y98">arXiv.org Snapshot					</li>
					<li id="item_6DRLED3U">ZhangFewshot2020.pdf					</li>
				</ul>
			</li>


			<li id="item_MSPDESXM" class="item journalArticle">
			<h2>Few-Shot Learning via Learning the Representation, Provably</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon S. Du</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wei Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sham M. Kakade</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason D. Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qi Lei</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2002.09434">http://arxiv.org/abs/2002.09434</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2002.09434 [cs, math, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-21</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2002.09434
Citation Key: DuFewShot2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/5/2020, 2:03:14 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper studies few-shot learning via representation learning, where one uses $T$ source tasks with $n_1$ data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only $n_2 (\ll n_1)$ data. Specifically, we focus on the setting where there exists a good \emph{common representation} between source and target, and our goal is to understand how much of a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a fast rate of $O\left(\frac{\mathcal{C}\left(\Phi\right)}{n_1T} + \frac{k}{n_2}\right)$; here, $\Phi$ is the representation function class, $\mathcal{C}\left(\Phi\right)$ is its complexity measure, and $k$ is the dimension of the representation. When specialized to linear representation functions, this rate becomes $O\left(\frac{dk}{n_1T} + \frac{k}{n_2}\right)$ where $d (\gg k)$ is the ambient input dimension, which is a substantial improvement over the rate without using representation learning, i.e. over the rate of $O\left(\frac{d}{n_2}\right)$. Second, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural network learning. Our results demonstrate representation learning can fully utilize all $n_1T$ samples from source tasks.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/5/2020, 2:03:14 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:22 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Mathematics - Optimization and Control</li>
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TWIGBT56">arXiv.org Snapshot					</li>
					<li id="item_6F2ZREJJ">DuFewShot2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_6F2ZREJJ/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/3">2 Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_6F2ZREJJ/3">3 Notation and Setup</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/3">3.1 Notation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/4">3.2 Problem Setup</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_6F2ZREJJ/5">4 Low-Dimensional Linear Representations</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/7">4.1 Proof Sketch of Theorem 4.1</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/8">5 General Low-Dimensional Representations</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_6F2ZREJJ/10">6 High-Dimensional Linear Representations</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/12">6.1 Proof Sketch for a Fixed Design</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/12">7 Neural Networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/14">8 Conclusion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_6F2ZREJJ/17">A Proof of Theorem 4.1</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/23">A.1 Technical Lemmas</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/25">B Proof of Theorem 5.1</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_6F2ZREJJ/28">C Proof of Theorem 6.1</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/32">C.1 Technical Lemmas</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_6F2ZREJJ/36">D Proof of Theorem 7.1</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_5DBFSY2G" class="item report">
			<h2>Hierarchical sparse coding of objects in deep convolutional neural networks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Report</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xingyu Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zonglei Zhen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jia Liu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://biorxiv.org/lookup/doi/10.1101/2020.06.29.176032">http://biorxiv.org/lookup/doi/10.1101/2020.06.29.176032</a></td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-06-29</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.1101/2020.06.29.176032</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>7/2/2020, 10:06:16 PM</td>
					</tr>
					<tr>
					<th>Institution</th>
						<td>Neuroscience</td>
					</tr>
					<tr>
					<th>Report Type</th>
						<td>preprint</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recently, deep convolutional neural networks (DCNNs) have attained human-level performances on challenging object recognition tasks owing to their complex internal representation. However, it remains unclear how objects are represented in DCNNs with an overwhelming number of features and non-linear operations. In parallel, the same question has been extensively studied in primates’ brain, and three types of coding schemes have been found: one object is coded by entire neuronal population (distributed coding), or by one single neuron (local coding), or by a subset of neuronal population (sparse coding). Here we asked whether DCNNs adopted any of these coding schemes to represent objects. Specifically, we used the population sparseness index, which is widely-used in neurophysiological studies on primates’ brain, to characterize the degree of sparseness at each layer in two representative DCNNs pretrained for object categorization, AlexNet and VGG11. We found that the sparse coding scheme was adopted at all layers of the DCNNs, and the degree of sparseness increased along the hierarchy. That is, the coding scheme shifted from distributed-like coding at lower layers to local-like coding at higher layers. Further, the degree of sparseness was positively correlated with DCNNs’ performance in object categorization, suggesting that the coding scheme was related to behavioral performance. Finally, with the lesion approach, we demonstrated that both external learning experiences and built-in gating operations were necessary to construct such a hierarchical coding scheme. In sum, our study provides direct evidence that DCNNs adopted a hierarchically-evolved sparse coding scheme as the biological brain does, suggesting an implementation-independent principle of representing a myriad of objects efficiently.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/2/2020, 10:06:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/2/2020, 10:06:17 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_68Y9Q6DZ">Liu et al. - 2020 - Hierarchical sparse coding of objects in deep conv.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_68Y9Q6DZ/2">Abstract</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/2">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_68Y9Q6DZ/4">2 Materials and methods</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/4">2.1 Visual images datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/4">2.2 DCNNs and activation extraction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/5">2.3 Population sparseness index</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/5">2.4 Relationship between population sparseness and classification performance</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/6">3 Results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/10">4 Discussion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/12">References</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_68Y9Q6DZ/15">Supplementary Material</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_PHQ8L6PG" class="item journalArticle">
			<h2>Improved Baselines with Momentum Contrastive Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xinlei Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haoqi Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ross Girshick</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaiming He</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.04297">http://arxiv.org/abs/2003.04297</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.04297 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-09</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.04297
Citation Key: ChenImproved2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/24/2020, 9:02:11 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR&apos;s design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/24/2020, 9:02:11 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_9ESHCV3R">
<p class="plaintext">Comment: Tech report, 2 pages + references</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EEFTUBMJ">arXiv.org Snapshot					</li>
					<li id="item_3RMSII55">ChenImproved2020.pdf					</li>
				</ul>
			</li>


			<li id="item_SYVJL87T" class="item conferencePaper">
			<h2>Invariant information clustering for unsupervised image classification and segmentation</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xu Ji</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>João F. Henriques</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Vedaldi</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>9865–9874</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: JiInvariant2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the IEEE International Conference on Computer Vision</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:52:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IKB9S83I">JiInvariant2019.pdf					</li>
					<li id="item_J2TCZTHW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VJVCWAU9" class="item conferencePaper">
			<h2>Joint Detection and Identification Feature Learning for Person Search</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tong Xiao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shuang Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bochao Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liang Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaogang Wang</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Xiao_Joint_Detection_and_CVPR_2017_paper.html">http://openaccess.thecvf.com/content_cvpr_2017/html/Xiao_Joint_Detection_and_CVPR_2017_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3415-3424</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: XiaoJoint2017</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/15/2020, 3:24:15 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/15/2020, 3:24:15 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:16 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IE73J6A4">Snapshot					</li>
					<li id="item_P7GKJ8ES">XiaoJoint2017.pdf					</li>
				</ul>
			</li>


			<li id="item_3MBMWZX8" class="item journalArticle">
			<h2>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yang You</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jing Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sashank Reddi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Hseu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjiv Kumar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Srinadh Bhojanapalli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaodan Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Demmel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kurt Keutzer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cho-Jui Hsieh</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1904.00962">http://arxiv.org/abs/1904.00962</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1904.00962 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-03</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1904.00962
Citation Key: YouLarge2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/21/2020, 12:06:05 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Large Batch Optimization for Deep Learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/21/2020, 12:06:06 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_KKJ8L9X9">
<p class="plaintext">Comment: Published as a conference paper at ICLR 2020</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LIVVDDZS">arXiv.org Snapshot					</li>
					<li id="item_9GSTGMCQ">YouLarge2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_9GSTGMCQ/1">1 Introduction</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/2">1.1 Related Work</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/3">2 Preliminaries</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_9GSTGMCQ/4">3 Algorithms</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/4">3.1 Lars Algorithm</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/5">3.2 Lamb Algorithm</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_9GSTGMCQ/6">4 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/6">4.1 Bert Training</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/7">4.2 ImageNet Training with ResNet-50.</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/8">4.3 Hyperparameters for scaling the batch size</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/8">5 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/9">6 Acknowledgement</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/11">A Proof of Theorem ??</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/12">B Proof of Theorem ??</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/14">C Comparison of Convergence Rates of Lars and Sgd</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/15">D N-LAMB: Nesterov Momentum for LAMB</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/15">E LAMB with learning rate correction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/16">F LAMB with different norms</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/16">G Regular Batch Sizes for Small Datasets: MNIST and CIFAR-10.</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_9GSTGMCQ/17">H Implementation Details and Additional Results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_9GSTGMCQ/17">H.0.1 BERT</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/18">H.0.2 ImageNet</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_9GSTGMCQ/18">H.1 Baseline tuning details for ImageNet training with ResNet-50</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_7P6TXBVE" class="item journalArticle">
			<h2>Large Batch Training of Convolutional Networks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yang You</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Igor Gitman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boris Ginsburg</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1708.03888">http://arxiv.org/abs/1708.03888</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1708.03888 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017-09-13</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1708.03888
Citation Key: YouLarge2017</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/21/2020, 12:04:40 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/21/2020, 12:04:40 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_W5ATXY67">arXiv.org Snapshot					</li>
					<li id="item_PL2AWJH9">YouLarge2017.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_PL2AWJH9/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/2">2 Background</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/3">3 Analysis of Alexnet training with large batch</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/4">4 Layer-wise Adaptive Rate Scaling (LARS)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/5">5 Training with LARS</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/7">6 Large Batch vs Number of steps</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PL2AWJH9/7">7 Conclusion</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_ALTSUV9L" class="item bookSection">
			<h2>Large Scale Adversarial Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeff Donahue</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Karen Simonyan</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Larochelle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>A. Beygelzimer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>F. d\textquotesingle Alché-Buc</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>E. Fox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/9240-large-scale-adversarial-representation-learning.pdf">http://papers.nips.cc/paper/9240-large-scale-adversarial-representation-learning.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>10542–10552</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: DonahueLarge2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 4:19:33 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 32</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 4:19:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IUAS9GRK">DonahueLarge2019.pdf					</li>
					<li id="item_RCKKTVAQ">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_CS4B49RD" class="item journalArticle">
			<h2>Large scale representation learning from triplet comparisons</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siavash Haghiri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leena Chennuru Vankadara</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ulrike von Luxburg</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1912.01666">http://arxiv.org/abs/1912.01666</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1912.01666 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-12-03</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1912.01666
Citation Key: HaghiriLarge2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/24/2020, 12:04:26 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we discuss the fundamental problem of representation learning from a new perspective. It has been observed in many supervised/unsupervised DNNs that the final layer of the network often provides an informative representation for many tasks, even though the network has been trained to perform a particular task. The common ingredient in all previous studies is a low-level feature representation for items, for example, RGB values of images in the image context. In the present work, we assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? We provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. This problem has been studied in a sub-community of machine learning by the name &quot;Ordinal Embedding&quot;. Previous approaches to the problem are painfully slow and cannot scale to larger datasets. We demonstrate that our proposed approach is significantly faster than available methods, and can scale to real-world large datasets. Thereby, we also draw attention to the less explored idea of using neural networks to directly, approximately solve non-convex, NP-hard optimization problems that arise naturally in unsupervised learning problems.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/24/2020, 12:04:26 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_U9F3EUNA">arXiv.org Snapshot					</li>
					<li id="item_M3WGBCK2">HaghiriLarge2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_M3WGBCK2/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/3">2 Background on comparison-based machine learning and ordinal embedding</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/4">3 Proposed method: ordinal embedding neural network (OENN)</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_M3WGBCK2/5">4 Simulations</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/5">4.1 Choice of hyper-parameters.</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/5">4.2 Choice of the length of input encoding.</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/6">4.3 Reconstruction and generalization to unseen triplets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/7">4.4 Comparison with other ordinal embedding methods</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/8">5 Experiments</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/8">6 Discussion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_M3WGBCK2/12">A Appendix</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/12">A.1 Dependence between the layer width and the number of items</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/12">A.2 Dependence between layer width and embedding dimension</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/13">A.3 Dependence between the size of the input encoding and the number of points</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/13">A.4 Dependence on size of the Input representation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/13">A.5 Reconstruction, detailed results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/15">A.6 Extra datasets, pathological examples</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/15">A.7 Details of the MTurk experiment</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3WGBCK2/17">A.8 The choice of the loss function</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_6EIWLW5M" class="item journalArticle">
			<h2>Learning deep representations by mutual information estimation and maximization</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Devon Hjelm</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Fedorov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Lavoie-Marchildon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Karan Grewal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phil Bachman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam Trischler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1808.06670</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: HjelmLearning2018</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:53:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4LCBQI5I">HjelmLearning2018.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_4LCBQI5I/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/2">2 Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_4LCBQI5I/3">3 Deep InfoMax</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/3">3.1 Mutual information estimation and maximization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/5">3.2 Local mutual information maximization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/5">3.3 Matching representations to a prior distribution</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_4LCBQI5I/6">4 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/6">4.1 How do we evaluate the quality of a representation?</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/7">4.2 Representation learning comparison across models</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/9">4.3 Adding coordinate information and occlusions</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/10">5 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/10">6 Acknowledgements</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_4LCBQI5I/15">A Appendix</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/15">A.1 On the Jensen-Shannon divergence and mutual information</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/15">A.2 Experiment and architecture details</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/18">A.3 Sampling strategies</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/19">A.4 Nearest-neighbor analysis</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/20">A.5 Ablation studies</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/21">A.6 Empirical consistency of Neural Dependency Measure (NDM)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/22">A.7 Additional details on occlusion and coordinate prediction experiments</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/23">A.8 Training a generator by matching to a prior implicitly</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/24">A.9 Generation experiments and results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_4LCBQI5I/24">A.10 Images Generation</a></li></ul></li></ul></div>
					</div>					</li>
					<li id="item_WDYYFTQI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ZMUUR59F" class="item journalArticle">
			<h2>Learning Image Decompositions with Hierarchical Sparse Coding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew D Zeiler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rob Fergus</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2010</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This scheme makes it possible to robustly learn multiple layers of representation and we show a model with 4 layers, trained on images from the Caltech101 dataset. We use our model to produce image decompositions that, when used as input to standard classiﬁcation schemes, give a signiﬁcant performance gain over low-level edge features and yield an overall performance competitive with leading approaches.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/2/2020, 10:06:20 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/2/2020, 10:06:20 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_86QFI59N">Zeiler and Fergus - 2010 - Learning Image Decompositions with Hierarchical Sp.pdf					</li>
				</ul>
			</li>


			<li id="item_JX8VEGTN" class="item conferencePaper">
			<h2>Learning representations by maximizing mutual information across views</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Bachman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Devon Hjelm</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Buchwalter</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>15509–15519</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BachmanLearning2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Advances in Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:36:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:29 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RZAVF239">BachmanLearning2019.pdf					</li>
					<li id="item_K4AZEBHI">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2Q98V3B8" class="item journalArticle">
			<h2>Learning Video Representations using Contrastive Bidirectional Transformer</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chen Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabien Baradel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Murphy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cordelia Schmid</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1906.05743">http://arxiv.org/abs/1906.05743</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1906.05743 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-09-27</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1906.05743
Citation Key: SunLearning2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:44:22 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:44:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y9ZGMEB5">arXiv.org Snapshot					</li>
					<li id="item_YVTP8QF7">SunLearning2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_YVTP8QF7/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/2">2 Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_YVTP8QF7/3">3 Method</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/3">3.1 The BERT model</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/4">3.2 The CBT model</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/4">3.3 The Cross-modal CBT model</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/5">3.4 Overall model</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_YVTP8QF7/5">4 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/5">4.1 Learning self-supervised visual representations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/6">4.2 Learning self-supervised temporal representations</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/8">5 Conclusion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_YVTP8QF7/12">6 Supplementary materials</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/12">6.1 Video Captioning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_YVTP8QF7/12">6.2 Action Segmentation</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_AP3SZ8LG" class="item conferencePaper">
			<h2>Local aggregation for unsupervised learning of visual embeddings</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chengxu Zhuang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Lin Zhai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Yamins</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>6002–6012</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: ZhuangLocal2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the IEEE International Conference on Computer Vision</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2020, 4:53:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/29/2020, 12:46:01 AM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QHPQ6FM4">Snapshot					</li>
					<li id="item_HB3VNEPU">ZhuangLocal2019.pdf					</li>
				</ul>
			</li>


			<li id="item_KX7JV6VL" class="item bookSection">
			<h2>Mean Field Residual Networks: On the Edge of Chaos</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ge Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Schoenholz</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>I. Guyon</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>U. V. Luxburg</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>S. Bengio</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Fergus</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>S. Vishwanathan</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos.pdf">http://papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>7103–7114</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: YangMean2017</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/9/2020, 3:15:32 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 30</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Mean Field Residual Networks</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/9/2020, 3:15:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:20 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QTBQ9IFY">NIPS Snapshot					</li>
					<li id="item_8XLM2RUR">YangMean2017.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_8XLM2RUR/1">Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/2">Background</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/3">Preliminaries</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/4">Overview</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/5">Theoretical Results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/6">Tanh</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/8">-ReLU</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/9">Experimental Results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/10">Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Additional Figures</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">A Listing of Main Theorems</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Tanh</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Reduced Residual Network</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Full Residual Network</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">-ReLU</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Full Residual Network</a></li></ul></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Proofs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Preliminary Lemmas</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Dynamics Zoo</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Forward Dynamical Equations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Backward Dynamical Equations</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Tanh: Reduced Residual Network</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Forward Dynamics</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Backward Dynamics</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Tanh: Full Residual Network</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Forward Dynamics</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Backward Dynamics</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_8XLM2RUR/1">-ReLU: Full Residual Network</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Forward Dynamics</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_8XLM2RUR/1">Backward Dynamics</a></li></ul></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_P6WGRAGY" class="item bookSection">
			<h2>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antti Tarvainen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Harri Valpola</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>I. Guyon</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>U. V. Luxburg</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>S. Bengio</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Fergus</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>S. Vishwanathan</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1195–1204</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/30/2020, 9:52:07 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 30</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Mean teachers are better role models</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/30/2020, 9:52:07 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/30/2020, 9:52:13 AM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_W9NGP5AV">NIPS Full Text PDF					</li>
					<li id="item_NHPEANGW">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_UPP3GA4K" class="item journalArticle">
			<h2>Metric learning: cross-entropy vs. pairwise losses</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Malik Boudiaf</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jérôme Rony</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Imtiaz Masud Ziko</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Granger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marco Pedersoli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pablo Piantanida</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ismail Ben Ayed</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.08983">http://arxiv.org/abs/2003.08983</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.08983 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-19</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.08983
version: 1
Citation Key: BoudiafMetric2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 7:59:24 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses and convoluted sample-mining and implementation strategies to ease optimization. The standard cross-entropy loss for classification has been largely overlooked in DML. On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances. However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances. As a result, minimizing the cross-entropy can be seen as an approximate bound-optimization (or Majorize-Minimize) algorithm for minimizing this pairwise loss. Second, we show that, more generally, minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. These findings indicate that the cross-entropy represents a proxy for maximizing the mutual information -- as pairwise losses do -- without the need for complex sample-mining and optimization schemes. Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships. Our experiments over four standard DML benchmarks (CUB200, Cars-196, Stanford Online Product and In-Shop) strongly support our findings. We consistently obtained state-of-the-art results, outperforming many recent and complex DML methods.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Metric learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 7:59:24 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_3FBVR5BR">
<p class="plaintext">Comment: 25 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y9BNEXX2">arXiv.org Snapshot					</li>
					<li id="item_T3VPXZEV">BoudiafMetric2020.pdf					</li>
				</ul>
			</li>


			<li id="item_GT9E6WH2" class="item journalArticle">
			<h2>Mine: mutual information neural estimation</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohamed Ishmael Belghazi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aristide Baratin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sai Rajeswar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sherjil Ozair</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron Courville</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Devon Hjelm</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1801.04062</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BelghaziMine2018</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Mine</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:53:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IABP4NII">BelghaziMine2018.pdf					</li>
					<li id="item_8YE62AFS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CTYYRS93" class="item bookSection">
			<h2>MixMatch: A Holistic Approach to Semi-Supervised Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Berthelot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Carlini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ian Goodfellow</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolas Papernot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Avital Oliver</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Colin A Raffel</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Larochelle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>A. Beygelzimer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>F. d\textquotesingle Alché-Buc</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>E. Fox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf">http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>5049–5059</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BerthelotMixMatch2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 7:49:46 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 32</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>MixMatch</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 7:49:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PKI64YD2">BerthelotMixMatch2019.pdf					</li>
					<li id="item_QBYUV8IT">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_8KE92UJV" class="item journalArticle">
			<h2>Modularizing Deep Learning via Pairwise Learning With Kernels</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shiyu Duan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shujian Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jose Principe</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2005.05541">http://arxiv.org/abs/2005.05541</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2005.05541 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-05-12</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2005.05541</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>7/3/2020, 10:36:50 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>By redefining the conventional notions of layers, we present an alternative view on finitely wide, fully trainable deep neural networks as stacked linear models in feature spaces, leading to a kernel machine interpretation. Based on this construction, we then propose a provably optimal modular learning framework for classification, avoiding between-module backpropagation. This modular training approach brings new insights into the label requirement of deep learning: It leverages weak pairwise labels when learning the hidden modules. When training the output module, on the other hand, it requires full supervision but achieves high label efficiency, needing as few as 10 randomly selected labeled examples (one from each class) to achieve 94.88\% accuracy on CIFAR-10 using a ResNet-18 backbone. Moreover, modular training enables fully modularized deep learning workflows, which then simplify the design and implementation of pipelines and improve the maintainability and reusability of models. To showcase the advantages of such a modularized workflow, we describe a simple yet reliable method for estimating reusability of pre-trained modules as well as task transferability in a transfer learning setting. At practically no computation overhead, it precisely described the task space structure of 15 binary classification tasks from CIFAR-10.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/3/2020, 10:36:50 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/3/2020, 10:36:50 AM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FHBVWJEA">arXiv.org Snapshot					</li>
					<li id="item_XNBGH4HQ">DuanModularizing2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_XNBGH4HQ/1">I Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/2">II Notations and Background</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/2">II-A Notations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/2">II-B Kernels and Kernel Machines</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/3">III Revealing the Hidden Kernel Machines in Neural Networks</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/3">III-A The Methodology</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/3">III-A1 Fully-Connected Neural Networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/3">III-A2 Convolutional Neural Networks</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/4">III-B Implementable Feature Maps and Linear Runtime</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/4">IV Provably Optimal Modular Learning</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/4">IV-A Set-Up, Goal, and an Idea</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/4">IV-B Main Result</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/5">IV-C Applicability of the Main Result</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/5">IV-C1 A Two-Module View on Neural Networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/5">IV-C2 Loss Functions</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/5">IV-D From Theory to Algorithm</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">V A Method for Task Transferability Estimation</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">VI Related Work</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">VI-A Connecting Neural Networks With Kernel Methods</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">VI-A1 Exact Equivalences</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">VI-A2 Equivalences in Infinite Widths and/or in Expectation</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/6">VI-B Modularized Deep Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/7">VI-C Task Transferability</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XNBGH4HQ/7">VII Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/7">VII-A Sanity Check: Modular Training Results in Identical Learning Dynamics As End-to-End</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/8">VII-B Sanity Check: Proxy Objectives Align Well With Accuracy</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/8">VII-C Accuracy on MNIST and CIFAR-10</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/9">VII-D Label Efficiency of Modular Deep Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/9">VII-E Connections With Un/Semi-Supervised Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/10">VII-F Transferability Estimation With Proxy Objective</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/10">VIII Conclusions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/10">IX Acknowledgements</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/10">References</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XNBGH4HQ/13">Appendix</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_GMYEHZ9Y" class="item journalArticle">
			<h2>Momentum contrast for unsupervised visual representation learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaiming He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haoqi Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuxin Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saining Xie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ross Girshick</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1911.05722</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: HeMomentum2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:49:24 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KRVT3NWT">HeMomentum2019.pdf					</li>
					<li id="item_8JKTMNSY">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MSCY2GF3" class="item conferencePaper">
			<h2>Mutual information neural estimation</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohamed Ishmael Belghazi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aristide Baratin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sai Rajeshwar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sherjil Ozair</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshua Bengio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron Courville</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Devon Hjelm</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>531–540</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BelghaziMutual2018</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>International Conference on Machine Learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:53:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IL3WFW76">BelghaziMutual2018.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_IL3WFW76/1">Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_IL3WFW76/2">Background</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/2">Mutual Information</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/2">Dual representations of the KL-divergence.</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_IL3WFW76/3">The Mutual Information Neural Estimator</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/3">Method</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/3">Correcting the bias from the stochastic gradients</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_IL3WFW76/3">Theoretical properties</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/3">Consistency</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/4">Sample complexity</a></li></ul></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_IL3WFW76/4">Empirical comparisons</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/4">Comparing MINE to non-parametric estimation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/4">Capturing non-linear dependencies</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_IL3WFW76/5">Applications</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/5">Maximizing mutual information to improve GANs</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/6">Maximizing mutual information to improve inference in bi-directional adversarial models</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/7">Information Bottleneck</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/8">Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_IL3WFW76/8">Acknowledgements</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_D2ZLPNCQ" class="item journalArticle">
			<h2>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ben Mildenhall</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pratul P. Srinivasan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew Tancik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan T. Barron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ravi Ramamoorthi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ren Ng</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.08934">http://arxiv.org/abs/2003.08934</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.08934 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-19</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.08934
version: 1
Citation Key: MildenhallNeRF2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 8:02:10 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>NeRF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 8:02:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:23 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Graphics</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_UXQFE9UX">
<p class="plaintext">Comment: Project page with videos and code: http://tancik.com/nerf</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KK6JNNCI">arXiv.org Snapshot					</li>
					<li id="item_IRU9NUAD">MildenhallNeRF2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_IRU9NUAD/1">NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_G29VW6PV" class="item journalArticle">
			<h2>Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael U. Gutmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aapo Hyvärinen</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://www.jmlr.org/papers/v13/gutmann12a.html">http://www.jmlr.org/papers/v13/gutmann12a.html</a></td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>Feb</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>307-361</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Machine Learning Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>ISSN 1533-7928</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2012</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: GutmannNoiseContrastive2012</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/25/2020, 9:54:43 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.jmlr.org</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/25/2020, 9:54:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:26 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P7IMX89Q">GutmannNoiseContrastive2012.pdf					</li>
					<li id="item_D8IALKDP">Snapshot					</li>
				</ul>
			</li>


			<li id="item_L2Z4DKFX" class="item conferencePaper">
			<h2>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Gutmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aapo Hyvarinen</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2010</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: GutmannNoisecontrastive2010</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>AISTATS</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation oﬀers the best trade-oﬀ between computational and statistical eﬃciency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random ﬁeld.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/14/2020, 4:20:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:16 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I3JYG4Z3">GutmannNoisecontrastive2010.pdf					</li>
				</ul>
			</li>


			<li id="item_P3CANJNG" class="item journalArticle">
			<h2>On Mutual Information Maximization for Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Tschannen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Josip Djolonga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul K. Rubenstein</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sylvain Gelly</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mario Lucic</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1907.13625">http://arxiv.org/abs/1907.13625</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1907.13625 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-23</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1907.13625
Citation Key: TschannenMutual2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 12:51:47 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 12:51:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_FSV26FR4">
<p class="plaintext">Comment: ICLR 2020. Michael Tschannen and Josip Djolonga contributed equally</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VP7FNX49">arXiv.org Snapshot					</li>
					<li id="item_GYBLJ62C">TschannenMutual2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_GYBLJ62C/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/2">2 Background and Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_GYBLJ62C/3">3 Biases in approximate information maximization</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/4">3.1 Large MI is not predictive of downstream performance</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/6">3.2 Higher capacity critics can lead to worse downstream performance</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/7">3.3 Encoder architecture can be more important than the specific estimator</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/7">4 Connection to deep metric learning and triplet losses</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/9">5 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/13">A Relation between (2) and the InfoMax objective</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/13">B Experiment details: Adversarially trained encoder (Section 3.1)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/13">C Connection between metric learning and InfoNCE</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/13">D InfoNCE under non-i.i.d. sampling</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/14">E Experiment details: Non-i.i.d. sampling (Section 4)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/15">F Additional Figures</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_GYBLJ62C/15">G Results for the experiments from Sec. 3.2 and 3.3 on CIFAR10</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_6EK4RCZF" class="item conferencePaper">
			<h2>On the importance of initialization and momentum in deep learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ilya Sutskever</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Martens</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>George Dahl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Hinton</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1139–1147</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2013</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: SutskeverImportance2013</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>International conference on machine learning</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/15/2020, 11:25:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:16 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MQJKQIFX">SutskeverImportance2013.pdf					</li>
				</ul>
			</li>


			<li id="item_WLHZ23I9" class="item journalArticle">
			<h2>Pre-trained Models for Natural Language Processing: A Survey</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xipeng Qiu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianxiang Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yige Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yunfan Shao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ning Dai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuanjing Huang</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.08271">http://arxiv.org/abs/2003.08271</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.08271 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-18</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.08271
version: 1
Citation Key: QiuPretrained2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 8:01:39 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Pre-trained Models for Natural Language Processing</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 8:01:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:23 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_I8WWEVGG">
<p class="plaintext">Comment: Invited Review of Science China Technological Sciences</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9FNI8MLP">arXiv.org Snapshot					</li>
					<li id="item_UEZ94TR2">QiuPretrained2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_UEZ94TR2/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/2">2 Background</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/2">2.1 Language Representation Learning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/2">2.2 Neural Contextual Encoders</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/3">2.3 Why Pre-training?</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/3">2.4 A Brief History of PTMs for NLP</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/4">2.4.1 Pre-trained word embeddings</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/4">2.4.2 Pre-trained contextual encoders</a></li></ul></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/4">3 Overview of PTMs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/4">3.1 Pre-training Tasks</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/5">3.1.1 Language Modeling (LM)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/5">3.1.2 Masked Language Modeling (MLM)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/6">3.1.3 Permuted Language Modeling (PLM)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/6">3.1.4 Denoising Autoencoder (DAE)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/6">3.1.5 Contrastive Learning (CTL)</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/7">3.1.6 Others</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/7">3.2 Taxonomy of PTMs</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/9">3.3 Model Analysis</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/9">3.3.1 Non-Contextual Embeddings</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/9">3.3.2 Contextual Embeddings</a></li></ul></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/10">4 Extensions of PTMs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/10">4.1 Knowledge-Enriched PTMs</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/10">4.2 Multi-Modal PTMs</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/11">4.3 Model Compression</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/11">4.3.1 Model Pruning</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/11">4.3.2 Quantization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/11">4.3.3 Parameter Sharing</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/11">4.3.4 Knowledge Distillation</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/12">4.4 Domain-Specific PTMs</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/12">4.5 Multilingual and Language-Specific PTMs</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/13">5 Adapting PTMs to Downstream Tasks</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/13">5.1 Transfer Learning</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/13">5.2 How to Transfer?</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/13">5.2.1 Choosing appropriate pre-training task, model architecture and corpus</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/13">5.2.2 Choosing appropriate layers</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/14">5.2.3 To tune or not to tune?</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/14">5.3 Fine-Tuning Strategies</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/14">6 Resources of PTMs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/15">6.1 Open-Source Implementations</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/15">6.2 Collections of Related Resources</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_UEZ94TR2/15">7 Applications</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/15">7.1 General Evaluation Benchmark</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/15">7.2 Machine Translation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/16">7.3 Question Answering</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/16">7.4 Sentiment Analysis</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/16">7.5 Summarization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/17">7.6 Named Entity Recgonition</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/17">8 Future Directions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_UEZ94TR2/18">9 Conclusion</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_M6KFNUKF" class="item journalArticle">
			<h2>Provable Guarantees for Gradient-Based Meta-Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mikhail Khodak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maria-Florina Balcan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ameet Talwalkar</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1902.10644">http://arxiv.org/abs/1902.10644</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1902.10644 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-05-16</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1902.10644
Citation Key: KhodakProvable2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/5/2020, 2:02:39 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between popular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good sample efficiency guarantees in the convex setting, with generalization bounds that improve with task-similarity, while also being computationally scalable to modern deep learning architectures and the many-task setting. Despite its simplicity, the algorithm matches, up to a constant factor, a lower bound on the performance of any such parameter-transfer method under natural task similarity assumptions. We use experiments in both convex and deep learning settings to verify and demonstrate the applicability of our theory.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/5/2020, 2:02:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:22 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_FLTQEHVJ">
<p class="plaintext">Comment: ICML 2019</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_96AKBTYM">arXiv.org Snapshot					</li>
					<li id="item_BZN2N42T">KhodakProvable2019.pdf					</li>
				</ul>
			</li>


			<li id="item_HLS5JT9Q" class="item journalArticle">
			<h2>Provable Representation Learning for Imitation Learning via Bi-level Optimization</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjeev Arora</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon S. Du</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sham Kakade</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuping Luo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikunj Saunshi</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2002.10544">http://arxiv.org/abs/2002.10544</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2002.10544 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-24</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2002.10544
Citation Key: AroraProvable2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/5/2020, 2:03:23 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multiple experts&apos; trajectories are available. We formulate representation learning as a bi-level optimization problem where the &quot;outer&quot; optimization tries to learn the joint representation and the &quot;inner&quot; optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theoretically, we show using our framework that representation learning can provide sample complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/5/2020, 2:03:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:22 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_EJ5R3EZN">
<p class="plaintext">Comment: 26 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L23GRVRD">AroraProvable2020.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_L23GRVRD/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/2">2 Related Work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/3">3 Preliminaries</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/5">4 Bi-level Optimization Framework</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_L23GRVRD/6">5 Representation Learning for Behavioral Cloning</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/8">5.1 Proof sketch</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/8">6 Representation Learning for Observations Alone Setting</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_L23GRVRD/11">7 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/12">7.1 Verification of Theory</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/13">7.2 Policy optimization with representations trained by imitation learning</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/13">8 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/16">A Proofs for Behavioral Cloning</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_L23GRVRD/18">B Proofs for Observation-Alone</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/19">B.1 Proof of Theorem B.1</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/20">B.2 Proof of Theorem B.2</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/21">B.3 Proofs of Lemmas</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_L23GRVRD/25">C Data Set Collection Details</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/25">C.1 Dataset from trajectories</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/25">C.2 Dataset from trajectories and interaction</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_L23GRVRD/25">D Experiment Details</a></li></ul></div>
					</div>					</li>
					<li id="item_AUVJDNAT">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_DRTRTBJK" class="item bookSection">
			<h2>Putting An End to End-to-End: Gradient-Isolated Learning of Representations</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sindy Löwe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter O\textquotesingle Connor</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bastiaan Veeling</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Larochelle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>A. Beygelzimer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>F. d\textquotesingle Alché-Buc</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>E. Fox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations.pdf">http://papers.nips.cc/paper/8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3039–3051</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: LowePutting2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/18/2020, 10:47:10 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 32</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Putting An End to End-to-End</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/18/2020, 10:47:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:30 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HC2L3G5X">LowePutting2019.pdf					</li>
					<li id="item_QKNJXWSC">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_7INCBMS2" class="item journalArticle">
			<h2>Representation Learning with Contrastive Predictive Coding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron van den Oord</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yazhe Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oriol Vinyals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1807.03748">http://arxiv.org/abs/1807.03748</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1807.03748 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-01-22</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1807.03748
Citation Key: OordRepresentation2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/18/2020, 11:20:00 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/18/2020, 11:20:01 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:30 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ID7T5L6I">arXiv.org Snapshot					</li>
					<li id="item_M3GDMMK3">OordRepresentation2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_M3GDMMK3/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_M3GDMMK3/2">2 Contrastive Predicting Coding</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/2">2.1 Motivation and Intuitions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/3">2.2 Contrastive Predictive Coding</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/3">2.3 InfoNCE Loss and Mutual Information Estimation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/4">2.4 Related Work</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_M3GDMMK3/4">3 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/4">3.1 Audio</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/6">3.2 Vision</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/7">3.3 Natural Language</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/9">3.4 Reinforcement Learning</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/9">4 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/9">5 Acknowledgements</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_M3GDMMK3/13">A Appendix</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_M3GDMMK3/13">A.1 Estimating the Mutual Information with InfoNCE</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_EV4UQ4DG" class="item journalArticle">
			<h2>Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yonglong Tian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yue Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dilip Krishnan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joshua B. Tenenbaum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phillip Isola</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2003.11539">http://arxiv.org/abs/2003.11539</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2003.11539 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-03-25</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2003.11539
version: 1
Citation Key: TianRethinking2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 8:00:37 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Rethinking Few-Shot Image Classification</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 8:00:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:23 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_9HBVHXAI">
<p class="plaintext">Comment: First two authors contributed equally. Code: http://github.com/WangYueFt/rfs/</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CGL6DE2Q">arXiv.org Snapshot					</li>
					<li id="item_7L6XXVK9">TianRethinking2020.pdf					</li>
				</ul>
			</li>


			<li id="item_WQQBFPJF" class="item conferencePaper">
			<h2>Revisiting Self-Supervised Visual Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Kolesnikov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaohua Zhai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucas Beyer</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html">http://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1920-1929</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: KolesnikovRevisiting2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:45:05 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:45:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NUSH8A4S">KolesnikovRevisiting2019.pdf					</li>
					<li id="item_CP6ILLRW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RVFJ2KN7" class="item conferencePaper">
			<h2>Scaling and Benchmarking Self-Supervised Visual Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Priya Goyal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dhruv Mahajan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abhinav Gupta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishan Misra</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.html">http://openaccess.thecvf.com/content_ICCV_2019/html/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>6391-6400</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: GoyalScaling2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:44:56 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE International Conference on Computer Vision</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:44:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8XKWLEP6">GoyalScaling2019.pdf					</li>
					<li id="item_KAHU2LHV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_6L4PXPTG" class="item webpage">
			<h2>Self-organizing neural network that discovers surfaces in random-dot stereograms</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Suzanna Becker</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Hinton</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nature.com/articles/355161a0">https://www.nature.com/articles/355161a0</a></td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: BeckerSelforganizing</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/31/2020, 7:40:08 PM</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/31/2020, 7:40:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PDJR7HT4">BeckerSelforganizing.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_PDJR7HT4/1">Self-organizing neural network that discovers surfaces in random-dot stereograms</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_LLK4T937" class="item conferencePaper">
			<h2>Self-Supervised Learning of Geometrically Stable Features Through Probabilistic Introspection</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Novotny</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Albanie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Diane Larlus</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Vedaldi</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Novotny_Self-Supervised_Learning_of_CVPR_2018_paper.html">http://openaccess.thecvf.com/content_cvpr_2018/html/Novotny_Self-Supervised_Learning_of_CVPR_2018_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3637-3645</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: NovotnySelfSupervised2018</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 11:43:24 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 11:43:23 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3IXQNH7P">NovotnySelfSupervised2018.pdf					</li>
					<li id="item_5DRJQHRW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_H8QGGVZT" class="item journalArticle">
			<h2>Self-supervised learning of pretext-invariant representations</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishan Misra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laurens van der Maaten</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1912.01991</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: MisraSelfsupervised2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:50:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YLU3MA3N">MisraSelfsupervised2019.pdf					</li>
					<li id="item_NISX8MTW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_M9WMBMTD" class="item journalArticle">
			<h2>Self-Supervised Learning of Video-Induced Visual Invariances</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Tschannen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Josip Djolonga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marvin Ritter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aravindh Mahendran</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neil Houlsby</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sylvain Gelly</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mario Lucic</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1912.02783">http://arxiv.org/abs/1912.02783</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1912.02783 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-12-05</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1912.02783
Citation Key: TschannenSelfSupervised2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/20/2020, 5:57:19 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We propose a general framework for self-supervised learning of transferable visual representations based on video-induced visual invariances (VIVI). We consider the implicit hierarchy present in the videos and make use of (i) frame-level invariances (e.g. stability to color and contrast perturbations), (ii) shot/clip-level invariances (e.g. robustness to changes in object orientation and lighting conditions), and (iii) video-level invariances (semantic relationships of scenes across shots/clips), to define a holistic self-supervised loss. Training models using different variants of the proposed framework on videos from the YouTube-8M data set, we obtain state-of-the-art self-supervised transfer learning results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only 1000 labels per task. We then show how to co-train our models jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points with 10x fewer labeled images, as well as the previous best supervised model by 3.7 points using the full ImageNet data set.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:57:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PX3VF589">arXiv.org Snapshot					</li>
					<li id="item_V48U7M35">TschannenSelfSupervised2019.pdf					</li>
				</ul>
			</li>


			<li id="item_HMCUJUZB" class="item journalArticle">
			<h2>Self-training with Noisy Student improves ImageNet classification</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qizhe Xie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh-Thang Luong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eduard Hovy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Quoc V. Le</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1911.04252">http://arxiv.org/abs/1911.04252</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1911.04252 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-07</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1911.04252
Citation Key: XieSelftraining2020</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/7/2020, 9:06:56 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/7/2020, 9:06:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:20 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MH2NXPNF">arXiv.org Snapshot					</li>
					<li id="item_9U9KFWU9">XieSelftraining2020.pdf					</li>
				</ul>
			</li>


			<li id="item_R2LQW3WR" class="item journalArticle">
			<h2>Selfie: Self-supervised Pretraining for Image Embedding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Trieu H. Trinh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh-Thang Luong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Quoc V. Le</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1906.02940">http://arxiv.org/abs/1906.02940</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1906.02940 [cs, eess, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-07-27</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1906.02940
Citation Key: TrinhSelfie2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:42:17 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce a pretraining technique called Selfie, which stands for SELFie supervised Image Embedding. Selfie generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other &quot;distractor&quot; patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selfie includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate Selfie on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying amounts of labeled data, from 5% to 100% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 x 224 with 60 examples per class (5%), our method improves the mean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across different runs.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Selfie</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:42:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
					<li>Electrical Engineering and Systems Science - Image and Video Processing</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8M87KRLY">arXiv.org Snapshot					</li>
					<li id="item_3VDIPRW4">TrinhSelfie2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_3VDIPRW4/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_3VDIPRW4/2">2 Method</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/2">2.1 Pretraining Details</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/5">2.2 Attention Pooling</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/5">2.3 Finetuning Details</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_3VDIPRW4/5">3 Experiments and Results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/5">3.1 Datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/6">3.2 Experimental setup</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/6">3.3 Results</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_3VDIPRW4/7">4 Analysis</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/7">4.1 Pretraining benefits more when there is less labeled data</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/8">4.2 Self-attention as the last layer helps finetuning performance.</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/9">5 Related Work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_3VDIPRW4/9">6 Conclusion</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_DEW9LECX" class="item journalArticle">
			<h2>Sgdr: Stochastic gradient descent with warm restarts</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ilya Loshchilov</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Frank Hutter</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv preprint arXiv:1608.03983</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: LoshchilovSgdr2016</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sgdr</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/22/2020, 9:46:43 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:13 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XZAL6CNE">LoshchilovSgdr2016.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_XZAL6CNE/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XZAL6CNE/3">2 Related Work</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/3">2.1 Restarts in gradient-free optimization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/3">2.2 Restarts in gradient-based optimization</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/4">3 Stochastic Gradient Descent with warm restarts (SGDR)</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XZAL6CNE/4">4 Experimental results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/4">4.1 Experimental settings</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/6">4.2 Single-Model Results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/7">4.3 Ensemble Results</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/8">4.4 Experiments on a dataset of EEG recordings</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/8">4.5 Preliminary experiments on a downsampled ImageNet dataset</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/10">5 Discussion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/10">6 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/11">7 Acknowledgments</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_XZAL6CNE/14">8 Supplementary Material</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_XZAL6CNE/14">8.1 50k vs 100k examples per epoch</a></li></ul></li></ul></div>
					</div>					</li>
					<li id="item_X9AI35GA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BX8S7CUD" class="item journalArticle">
			<h2>Test-Time Training for Out-of-Distribution Generalization</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yu Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaolong Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhuang Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Miller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexei A. Efros</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Moritz Hardt</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1909.13231">http://arxiv.org/abs/1909.13231</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1909.13231 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-10-25</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1909.13231
Citation Key: SunTestTime2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/23/2020, 12:48:29 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce a general approach, called test-time training, for improving the performance of predictive models when test and training data come from different distributions. Test-time training turns a single unlabeled test instance into a self-supervised learning problem, on which we update the model parameters before making a prediction on this instance. We show that this simple idea leads to surprising improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts. Theoretical investigations on a convex model reveal helpful intuitions for when we can expect our approach to help.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/23/2020, 12:48:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:27 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UJ85W48R">arXiv.org Snapshot					</li>
					<li id="item_PPDBJ2DY">SunTestTime2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_PPDBJ2DY/1">1 Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/2">2 Method</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_PPDBJ2DY/3">3 Empirical Results</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/4">3.1 Common Corruptions</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/5">3.2 Video Classification</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/6">3.3 CIFAR-10.1: A New Test Set With Unknown Distribution Shifts</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/6">4 Towards Understanding Test-time Training</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/7">5 Related Work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/9">6 Conclusion</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/15">A A Theoretical Discussion on Our Variable Decision Boundary</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_PPDBJ2DY/15">B Proofs</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/15">B.1 Proofs for the toy problem</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/16">B.2 Proof of maintheorem</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/17">C Computational Aspects of Our Method</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/17">D Sample Images from the Common Corruptions Benchmark</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_PPDBJ2DY/17">E Additional Results on the Common Corruptions Dataset</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/17">E.1 Results using Batch Normalization</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/18">E.2 Additional Baseline: Adversarial Logit Pairing</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/19">E.3 Results on CIFAR-10-C and ImageNet-C, level 5</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_PPDBJ2DY/19">E.4 Results on CIFAR-10-C, levels 1-4</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_I8UG2GBP" class="item journalArticle">
			<h2>Theoretical Insights into Contrastive Unsupervised Representation Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hrishikesh Khandeparkar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjeev Arora</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>15</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: KhandeparkarTheoretical</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Unsupervised Representation Learning has had a tremendous impact in several domains like computer vision and natural language processing (NLP). Contrastive unsupervised methods to learn representations of data points have led to a simpliﬁcation of downstream classiﬁcation tasks which use these representations. However, a theoretical understanding of when these methods will succeed is lacking. In this report, we use the framework presented in [Arora et al., 2019] to guide the construction of better training objectives which we further test with experiments. Furthermore, we highlight the limitations of unsupervised learning in the form of counter examples showing when unsupervised learning is doomed to fail. We verify our results by conducting experiments on commonly used function classes for classiﬁcation in the domains of computer vision.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/26/2020, 11:20:30 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EDIZQL59">KhandeparkarTheoretical.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_EDIZQL59/1">Introduction</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/3">A Theoretical Framework</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/6">Unsupervised Representation Learning</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_EDIZQL59/6">Algorithmic Insights</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/6">The Negative Effects of Excessive Negative Sampling</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/9">Utilizing Blocks of Similar Data</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/10">Experiments</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_EDIZQL59/13">Conclusion</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_IAF3MRJX" class="item journalArticle">
			<h2>Unsupervised Data Augmentation for Consistency Training</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qizhe Xie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zihang Dai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eduard Hovy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh-Thang Luong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Quoc V. Le</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1904.12848">http://arxiv.org/abs/1904.12848</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1904.12848 [cs, stat]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-09-30</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1904.12848
Citation Key: XieUnsupervised2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/30/2020, 12:21:34 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/30/2020, 12:21:34 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZMTCP578">arXiv.org Snapshot					</li>
					<li id="item_E7C4GSZZ">XieUnsupervised2019.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_E7C4GSZZ/1">1 Introduction</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_E7C4GSZZ/2">2 Unsupervised Data Augmentation (UDA)</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/2">2.1 Background: Supervised Data Augmentation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/3">2.2 Unsupervised Data Augmentation</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/4">2.3 Augmentation Strategies for Different Tasks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/5">2.4 Training Signal Annealing for Low-data Regime</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_E7C4GSZZ/5">3 Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/5">3.1 Correlation between Supervised and Semi-supervised Performances</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/6">3.2 Algorithm Comparison on Vision Semi-supervised Learning Benchmarks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/6">3.3 Evaluation on Text Classification Datasets</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/8">3.4 Scalability Test on the ImageNet Dataset</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/9">3.5 Ablation Studies for TSA</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/9">4 Related Work</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/9">5 Conclusion</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_E7C4GSZZ/14">A More Experiments</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/14">A.1 Ablations Studies on RandAugment</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/14">A.2 Results on CIFAR-10 and SVHN with varied label set sizes</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/15">B Additional Training Techniques</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/15">C Extended Augmentation Strategies for Different Tasks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/16">D Extended Related Work</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_E7C4GSZZ/17">E Experiment Details</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/17">E.1 Text Classifications</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/18">E.2 Semi-supervised learning benchmarks CIFAR-10 and SVHN</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_E7C4GSZZ/18">E.3 ImageNet</a></li></ul></li></ul></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_K477MS4N" class="item conferencePaper">
			<h2>Unsupervised embedding learning via invariant and spreading instance feature</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mang Ye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xu Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pong C. Yuen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shih-Fu Chang</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>6210–6219</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: YeUnsupervised2019</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:52:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_43FZMIWU">Snapshot					</li>
					<li id="item_XFEVJL6R">YeUnsupervised2019.pdf					</li>
				</ul>
			</li>


			<li id="item_F49P4FCE" class="item conferencePaper">
			<h2>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhirong Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuanjun Xiong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stella X. Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dahua Lin</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html">http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3733-3742</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: WuUnsupervised2018</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/20/2020, 5:49:29 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/20/2020, 5:49:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:28 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GK3ALRKQ">Snapshot					</li>
					<li id="item_JP6NMEZA">WuUnsupervised2018.pdf					</li>
				</ul>
			</li>


			<li id="item_2SBRYN4L" class="item journalArticle">
			<h2>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mathilde Caron</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishan Misra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julien Mairal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Priya Goyal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Piotr Bojanowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Armand Joulin</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2006.09882">http://arxiv.org/abs/2006.09882</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:2006.09882 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-06-18</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2006.09882</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>6/26/2020, 9:11:29 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Unsupervised image representations have signiﬁcantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efﬁcient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our ﬁndings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>6/26/2020, 9:11:29 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>6/26/2020, 9:11:29 AM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2AR6ILVL">Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf					</li>
				</ul>
			</li>


			<li id="item_Z68TSDAH" class="item bookSection">
			<h2>Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dan Hendrycks</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mantas Mazeika</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saurav Kadavath</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Song</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Wallach</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>H. Larochelle</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>A. Beygelzimer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>F. d\textquotesingle Alché-Buc</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>E. Fox</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>R. Garnett</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://papers.nips.cc/paper/9697-using-self-supervised-learning-can-improve-model-robustness-and-uncertainty.pdf">http://papers.nips.cc/paper/9697-using-self-supervised-learning-can-improve-model-robustness-and-uncertainty.pdf</a></td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Curran Associates, Inc.</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>15663–15674</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: HendrycksUsing2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:44:33 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Neural Information Processing Systems</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Neural Information Processing Systems 32</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:44:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:24 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CLEDWW7K">HendrycksUsing2019.pdf					</li>
					<li id="item_2EKWGBB8">NIPS Snapshot					</li>
				</ul>
			</li>


			<li id="item_RLJLMFTP" class="item conferencePaper">
			<h2>Video Representation Learning by Dense Predictive Coding</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tengda Han</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weidi Xie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrew Zisserman</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/HVU/Han_Video_Representation_Learning_by_Dense_Predictive_Coding_ICCVW_2019_paper.html">http://openaccess.thecvf.com/content_ICCVW_2019/html/HVU/Han_Video_Representation_Learning_by_Dense_Predictive_Coding_ICCVW_2019_paper.html</a></td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>0-0</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Citation Key: HanVideo2019</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/29/2020, 5:43:45 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>openaccess.thecvf.com</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE International Conference on Computer Vision Workshops</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/29/2020, 5:43:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/23/2020, 4:23:25 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7WQIYSH9">HanVideo2019.pdf					</li>
					<li id="item_8HK69FC6">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5VA6S8WH" class="item journalArticle">
			<h2>Why should we add early exits to neural networks?</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simone Scardapane</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michele Scarpiniti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Enzo Baccarelli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aurelio Uncini</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2004.12814">http://arxiv.org/abs/2004.12814</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Cognitive Computation</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1866-9956, 1866-9964</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-6-17</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 2004.12814</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Cogn Comput</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s12559-020-09734-4">10.1007/s12559-020-09734-4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>7/3/2020, 10:37:48 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Deep neural networks are generally designed as a stack of differentiable layers, in which a prediction is obtained only after running the full stack. Recently, some contributions have proposed techniques to endow the networks with early exits, allowing to obtain predictions at intermediate points of the stack. These multi-output networks have a number of advantages, including: (i) significant reductions of the inference time, (ii) reduced tendency to overfitting and vanishing gradients, and (iii) capability of being distributed over multi-tier computation platforms. In addition, they connect to the wider themes of biological plausibility and layered cognitive reasoning. In this paper, we provide a comprehensive introduction to this family of neural networks, by describing in a unified fashion the way these architectures can be designed, trained, and actually deployed in time-constrained scenarios. We also describe in-depth their application scenarios in 5G and Fog computing environments, as long as some of the open research questions connected to them.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>7/3/2020, 10:37:48 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>7/3/2020, 10:37:48 AM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Neural and Evolutionary Computing</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_W54E9S2L">
<p class="plaintext">Comment: Published in Cognitive Computation</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_USDCZNPN">arXiv.org Snapshot					</li>
					<li id="item_AA2LEQVX">ScardapaneWhy2020a.pdf						<div class="note"><div><p xmlns="http://www.w3.org/1999/xhtml" id="title"><strong>Contents</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style="list-style-type: none; padding-left:0px" id="toc"><li><a href="zotero://open-pdf/0_AA2LEQVX/1">1 Introduction</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/2">1.1 Contribution of this work</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/3">2 Related works</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/4">3 Description of the model</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/4">3.1 Basic neural networks</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/5">3.2 Neural networks with early exits</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/5">3.2.1 Selecting where and how to early-exit</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/6">3.2.2 Training the network</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/6">3.2.3 Exploiting multiple early exits in the inference phase</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/6">3.2.4 Formal properties of multi-exit networks</a></li></ul></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/7">4 Placement and optimized design of the early exits</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/8">5 Training neural networks with early exits</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/8">5.1 Joint training approaches</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/9">5.1.1 On the convergence property of the JT approach</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/9">5.2 Joint training on a combined output</a></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/10">5.3 Layer-wise training</a><ul style="list-style-type: none; padding-left:24px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/11">5.3.1 On the formal properties of the LT approach</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/11">5.4 Further training approaches</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/12">6 Inference in multi-exit networks</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/13">6.1 Regularizing for computational cost</a></li></ul></li><li style="padding-top:8px"><a href="zotero://open-pdf/0_AA2LEQVX/14">7 Additional topics</a><ul style="list-style-type: none; padding-left:12px"><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/14">7.1 Biological plausibility of multi-exit networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/15">7.2 Distributed implementation of multi-exit networks</a></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/16">7.3 The information bottleneck principle</a></li></ul></li><li style="padding-top:4px"><a href="zotero://open-pdf/0_AA2LEQVX/17">8 Open research challenges</a></li></ul></div>
					</div>					</li>
				</ul>
			</li>

		</ul>
	</body>
</html>